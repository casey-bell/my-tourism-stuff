from pathlib import Path
import json

nb = {
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x",
            "mimetype": "text/x-python",
            "codemirror_mode": {"name": "ipython", "version": 3},
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "cells": [
        {"cell_type": "markdown", "metadata": {}, "source": [
            "# Transform and validate visitor data\n\n",
            "This notebook performs the **transformation** and **validation** stages of the data workflow.\n\n",
            "**Objectives**\n\n",
            "- Load cleaned tabular data\n",
            "- Transform to a quarterly, tidy structure suitable for downstream analysis\n",
            "- Persist interim and processed artefacts\n",
            "- Run schema and rule-based validations, and surface a concise report"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 0. Setup"]},
        {"cell_type": "code", "metadata": {"execution": {"iopub.status.busy": ""}}, "execution_count": None, "outputs": [], "source": [
            "from __future__ import annotations\n",
            "from pathlib import Path\n",
            "import sys\n",
            "import json\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Ensure the project src/ is on the Python path for imports when running the notebook directly\n",
            "PROJECT_ROOT = Path.cwd().resolve()\n",
            "if (PROJECT_ROOT / 'src').exists() and str(PROJECT_ROOT / 'src') not in sys.path:\n",
            "    sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
            "\n",
            "# Display options for easier inspection\n",
            "pd.set_option('display.max_columns', 100)\n",
            "pd.set_option('display.width', 120)\n",
            "\n",
            "# Paths (prefer config if available)\n",
            "try:\n",
            "    from config import DATA_DIR  # src/config.py\n",
            "    DATA_DIR = Path(DATA_DIR)\n",
            "    RAW_DIR = DATA_DIR / 'raw'\n",
            "    INTERIM_DIR = DATA_DIR / 'interim'\n",
            "    PROCESSED_DIR = DATA_DIR / 'processed'\n",
            "except Exception:\n",
            "    DATA_DIR = PROJECT_ROOT / 'data'\n",
            "    RAW_DIR = DATA_DIR / 'raw'\n",
            "    INTERIM_DIR = DATA_DIR / 'interim'\n",
            "    PROCESSED_DIR = DATA_DIR / 'processed'\n",
            "\n",
            "RAW_FILE = RAW_DIR / 'overseas-visitors-to-britain-2024.xlsx'\n",
            "INTERIM_OUT = INTERIM_DIR / 'visitors_by_quarter.csv'\n",
            "PROCESSED_OUT = PROCESSED_DIR / 'visitors_quarterly.parquet'\n",
            "\n",
            "# Create output folders if missing\n",
            "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
            "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
            "\n",
            "print('Project root:', PROJECT_ROOT)\n",
            "print('Raw file:', RAW_FILE)\n",
            "print('Interim out:', INTERIM_OUT)\n",
            "print('Processed out:', PROCESSED_OUT)"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Import pipeline modules"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Import project modules\n",
            "from data import load, clean, transform, validate, schemas\n",
            "\n",
            "# Optional utility layer for IO if present\n",
            "try:\n",
            "    from utils import io as io_utils\n",
            "except Exception:\n",
            "    io_utils = None\n",
            "\n",
            "print('Modules imported: load, clean, transform, validate, schemas')"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load raw data"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Read from the ONS Excel workbook.\n",
            "# If load module exposes a specific function, prefer that; fallback to pandas.\n",
            "try:\n",
            "    df_raw = load.read_excel(RAW_FILE)\n",
            "except AttributeError:\n",
            "    # Generic fallback: first sheet\n",
            "    df_raw = pd.read_excel(RAW_FILE)\n",
            "\n",
            "print(df_raw.shape)\n",
            "df_raw.head()"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Clean and standardise"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Apply cleaning rules defined in src/data/clean.py\n",
            "try:\n",
            "    df_clean = clean.clean_visitors(df_raw)\n",
            "except AttributeError:\n",
            "    # Fallback: standardise column names minimally\n",
            "    df_clean = df_raw.copy()\n",
            "    df_clean.columns = [str(c).strip().lower().replace(' ', '_') for c in df_clean.columns]\n",
            "\n",
            "df_clean.info()\n",
            "df_clean.head()"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Transform to quarterly tidy dataset"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Aggregate/reshape to a tidy quarterly table\n",
            "try:\n",
            "    df_quarterly = transform.aggregate_quarterly(df_clean)\n",
            "except AttributeError:\n",
            "    # Fallback: assume already quarterly; ensure expected dtypes\n",
            "    df_quarterly = df_clean.copy()\n",
            "\n",
            "# Quick sense-check: show the latest few quarters\n",
            "df_quarterly.sort_values(by=df_quarterly.columns[:2].tolist()).tail(8)"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["### Optional: quick visual check"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Plot total visits by quarter if the relevant columns exist\n",
            "cols = [c.lower() for c in df_quarterly.columns]\n",
            "possible_quarter_cols = ['quarter', 'period', 'qtr', 'year_quarter']\n",
            "possible_visits_cols = ['visits', 'number_of_visits', 'visits_thousands']\n",
            "q_col = next((c for c in df_quarterly.columns if c.lower() in possible_quarter_cols), None)\n",
            "v_col = next((c for c in df_quarterly.columns if c.lower() in possible_visits_cols), None)\n",
            "if q_col and v_col:\n",
            "    ax = df_quarterly[[q_col, v_col]].set_index(q_col).sort_index()[v_col].plot(kind='line', figsize=(9, 4), title='Visits by quarter')\n",
            "    ax.set_xlabel('Quarter')\n",
            "    ax.set_ylabel('Visits')\n",
            "else:\n",
            "    print('Skipping chart: could not infer quarter/visits columns')"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Persist interim and processed artefacts"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Write CSV (interim)\n",
            "try:\n",
            "    if io_utils is not None and hasattr(io_utils, 'write_csv'):\n",
            "        io_utils.write_csv(df_quarterly, INTERIM_OUT)\n",
            "    else:\n",
            "        df_quarterly.to_csv(INTERIM_OUT, index=False)\n",
            "    print(f'Wrote: {INTERIM_OUT}')\n",
            "except Exception as e:\n",
            "    print('Failed to write interim CSV:', e)\n",
            "\n",
            "# Write Parquet (processed)\n",
            "try:\n",
            "    if io_utils is not None and hasattr(io_utils, 'write_parquet'):\n",
            "        io_utils.write_parquet(df_quarterly, PROCESSED_OUT)\n",
            "    else:\n",
            "        df_quarterly.to_parquet(PROCESSED_OUT, index=False)\n",
            "    print(f'Wrote: {PROCESSED_OUT}')\n",
            "except Exception as e:\n",
            "    print('Failed to write processed Parquet:', e)"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Validation"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Schema and business-rule checks\n",
            "validation_summary = {}\n",
            "\n",
            "# 6.1 Schema validation\n",
            "try:\n",
            "    quarterly_schema = getattr(schemas, 'QUARTERLY_VISITORS_SCHEMA')\n",
            "except Exception:\n",
            "    quarterly_schema = None\n",
            "\n",
            "try:\n",
            "    if quarterly_schema is not None and hasattr(validate, 'validate_schema'):\n",
            "        schema_report = validate.validate_schema(df_quarterly, quarterly_schema)\n",
            "    elif hasattr(validate, 'validate_quarterly'):\n",
            "        schema_report = validate.validate_quarterly(df_quarterly)\n",
            "    else:\n",
            "        schema_report = {'status': 'skipped', 'detail': 'No schema validator available'}\n",
            "    validation_summary['schema'] = schema_report\n",
            "except Exception as e:\n",
            "    validation_summary['schema'] = {'status': 'error', 'error': str(e)}\n",
            "\n",
            "# 6.2 Basic rule checks (non-negative metrics, no duplicate quarters per group, etc.)\n",
            "basic_checks = {}\n",
            "try:\n",
            "    # Example checks: replace column names as appropriate in your codebase\n",
            "    non_negative_cols = [c for c in df_quarterly.columns if any(k in c.lower() for k in ['visit', 'expenditure', 'nights'])]\n",
            "    negatives = {}\n",
            "    for c in non_negative_cols:\n",
            "        nneg = int((df_quarterly[c] < 0).sum()) if pd.api.types.is_numeric_dtype(df_quarterly[c]) else 0\n",
            "        if nneg:\n",
            "            negatives[c] = nneg\n",
            "    basic_checks['non_negative_fail_counts'] = negatives\n",
            "\n",
            "    # Duplicate key rows check: attempt to infer key columns\n",
            "    key_candidates = ['year', 'quarter', 'period', 'region', 'country', 'geography', 'purpose', 'transport']\n",
            "    keys = [c for c in df_quarterly.columns if c.lower() in key_candidates]\n",
            "    dup_count = int(df_quarterly.duplicated(subset=keys).sum()) if keys else 0\n",
            "    basic_checks['duplicate_rows_on_keys'] = dup_count\n",
            "\n",
            "    validation_summary['basic_rules'] = {'status': 'ok' if not negatives and dup_count == 0 else 'warn', 'detail': basic_checks}\n",
            "except Exception as e:\n",
            "    validation_summary['basic_rules'] = {'status': 'error', 'error': str(e)}\n",
            "\n",
            "validation_summary"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Assert pass criteria"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Define minimal pass criteria; adapt to your schema/rules as required\n",
            "status = {k: v.get('status', 'unknown') for k, v in validation_summary.items()}\n",
            "print('Validation status by category:', status)\n",
            "\n",
            "# Treat any explicit 'error' as a hard failure\n",
            "if any(v == 'error' for v in status.values()):\n",
            "    raise AssertionError('Validation errors detected. Inspect validation_summary above.')\n",
            "\n",
            "print('Validation completed.')"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Save validation report (optional)"]},
        {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": [
            "# Optionally persist a JSON validation report alongside artefacts\n",
            "report_path = PROCESSED_DIR / 'validation_report.json'\n",
            "try:\n",
            "    with open(report_path, 'w', encoding='utf-8') as f:\n",
            "        json.dump(validation_summary, f, indent=2, ensure_ascii=False)\n",
            "    print(f'Wrote: {report_path}')\n",
            "except Exception as e:\n",
            "    print('Skipped writing validation report:', e)"
        ]},
        {"cell_type": "markdown", "metadata": {}, "source": [
            "## 9. Next steps\n\n",
            "- Use the processed dataset for further analysis (e.g., `src/analysis/summary.py`).\n",
            "- Produce charts for reporting (e.g., `src/analysis/charts.py`).\n",
            "- Integrate this notebook into automated checks if desired."
        ]}
    ]
}

notebook_path = Path('notebooks') / '02-transform-and-validate.ipynb'
notebook_path.parent.mkdir(parents=True, exist_ok=True)
with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2)

print(f"Notebook written to: {notebook_path.resolve()}")
