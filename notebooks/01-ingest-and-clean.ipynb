{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## UK Tourism Data â€“ Ingestion and Cleaning Pipeline\n",
        "\n",
        "This notebook processes the official ONS tourism statistics, handling the 2024 methodological break and producing clean, ready-for-analysis data sets.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# Add src directory to path for utility functions\n",
        "SRC_DIR = Path.cwd().parent / \"src\"\n",
        "sys.path.append(str(SRC_DIR))\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Project Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Define project paths\n",
        "ROOT_DIR = Path.cwd().parent\n",
        "DATA_RAW = ROOT_DIR / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = ROOT_DIR / \"data\" / \"processed\"\n",
        "\n",
        "# Ensure directories exist\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Data file\n",
        "DATA_FILE = DATA_RAW / \"overseas-visitors-to-britain-2024.xlsx\"\n",
        "\n",
        "logger.info(f\"Project root: {ROOT_DIR}\")\n",
        "logger.info(f\"Data file: {DATA_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# First, inspect the Excel file structure\n",
        "def inspect_excel_file(file_path: Path):\n",
        "    \"\"\"Inspect Excel file structure without loading full data.\"\"\"\n",
        "    wb = load_workbook(file_path, read_only=True)\n",
        "    sheets = wb.sheetnames\n",
        "    logger.info(f\"Available sheets: {sheets}\")\n",
        "    \n",
        "    # Sample first few rows of each sheet\n",
        "    for sheet in sheets:\n",
        "        ws = wb[sheet]\n",
        "        headers = [cell.value for cell in next(ws.iter_rows(max_row=1))]\n",
        "        logger.info(f\"Sheet '{sheet}' - Columns: {headers[:5]}...\")  # First 5 columns\n",
        "    wb.close()\n",
        "    return sheets\n",
        "\n",
        "sheets = inspect_excel_file(DATA_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Validate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the main data sheet\n",
        "try:\n",
        "    df_raw = pd.read_excel(\n",
        "        DATA_FILE,\n",
        "        sheet_name=0,  # Assuming first sheet contains main data\n",
        "        engine='openpyxl'\n",
        "    )\n",
        "    logger.info(f\"Successfully loaded data: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Initial data quality check\n",
        "def initial_validation(df: pd.DataFrame):\n",
        "    \"\"\"Perform initial data validation.\"\"\"\n",
        "    validation_results = {\n",
        "        'total_rows': len(df),\n",
        "        'total_columns': len(df.columns),\n",
        "        'missing_values': df.isnull().sum().sum(),\n",
        "        'duplicate_rows': df.duplicated().sum()\n",
        "    }\n",
        "    \n",
        "    # Check for expected columns\n",
        "    expected_cols = ['Quarter', 'Visits', 'Spending', 'Nights stayed']\n",
        "    found_cols = [col for col in expected_cols if col in df.columns]\n",
        "    validation_results['expected_columns_found'] = len(found_cols)\n",
        "    \n",
        "    logger.info(\"Initial validation results:\")\n",
        "    for key, value in validation_results.items():\n",
        "        logger.info(f\"  {key}: {value}\")\n",
        "    \n",
        "    return validation_results\n",
        "\n",
        "validation = initial_validation(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Standardise column names.\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Create mapping for column renaming\n",
        "    column_mapping = {\n",
        "        'Quarter': 'quarter',\n",
        "        'Visits': 'num_visits',\n",
        "        'Spending': 'spend_gbp_millions', \n",
        "        'Nights stayed': 'num_nights',\n",
        "        # Add other column mappings as needed\n",
        "    }\n",
        "    \n",
        "    # Rename columns\n",
        "    df_clean = df_clean.rename(columns=column_mapping)\n",
        "    \n",
        "    # Clean any remaining columns\n",
        "    def standardise_name(name: str) -> str:\n",
        "        if name in column_mapping.values():\n",
        "            return name\n",
        "        return (\n",
        "            str(name)\n",
        "            .strip()\n",
        "            .lower()\n",
        "            .replace(' ', '_')\n",
        "            .replace('-', '_')\n",
        "            .replace('/', '_')\n",
        "        )\n",
        "    \n",
        "    df_clean.columns = [standardise_name(col) for col in df_clean.columns]\n",
        "    return df_clean\n",
        "\n",
        "df_clean = clean_column_names(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Parse Temporal Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def parse_quarter_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Parse quarter information into a proper datetime period index.\"\"\"\n",
        "    df_temp = df.copy()\n",
        "    \n",
        "    # Handle different quarter formats\n",
        "    if 'quarter' in df_temp.columns:\n",
        "        # Extract year and quarter (assumes formats like 'Q1 2024' or '1 2024')\n",
        "        quarter_data = df_temp['quarter'].astype(str).str.extract(r'Q?(\\d)\\s*(\\d{4})')\n",
        "        \n",
        "        if not quarter_data.empty:\n",
        "            df_temp['year'] = quarter_data[1].astype(int)\n",
        "            df_temp['quarter_num'] = quarter_data[0].astype(int)\n",
        "            \n",
        "            # Create period index as quarterly periods\n",
        "            df_temp['period'] = df_temp.apply(\n",
        "                lambda x: pd.Period(f\"{x['year']}Q{x['quarter_num']}\", freq='Q'),\n",
        "                axis=1\n",
        "            )\n",
        "            df_temp = df_temp.set_index('period')\n",
        "            \n",
        "            # Add scope flag for the methodological break\n",
        "            df_temp['geographic_scope'] = df_temp.index.to_series().apply(\n",
        "                lambda p: 'GB' if p.year >= 2024 else 'UK'\n",
        "            )\n",
        "    \n",
        "    return df_temp\n",
        "\n",
        "df_processed = parse_quarter_data(df_clean)\n",
        "logger.info(f\"Data after temporal processing: {len(df_processed)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Handle Data Types and Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def enforce_data_types(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Ensure proper data types and handle missing values.\"\"\"\n",
        "    df_typed = df.copy()\n",
        "    \n",
        "    # Numeric columns with robust error handling\n",
        "    numeric_columns = ['num_visits', 'spend_gbp_millions', 'num_nights']\n",
        "    \n",
        "    for col in numeric_columns:\n",
        "        if col in df_typed.columns:\n",
        "            original_non_null = df_typed[col].notna().sum()\n",
        "            df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce')\n",
        "            new_non_null = df_typed[col].notna().sum()\n",
        "            \n",
        "            if new_non_null < original_non_null:\n",
        "                logger.warning(f\"Coerced {original_non_null - new_non_null} non-numeric values to NaN in {col}\")\n",
        "    \n",
        "    return df_typed\n",
        "\n",
        "df_final = enforce_data_types(df_processed)\n",
        "\n",
        "# Final data quality report\n",
        "def generate_quality_report(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Generate a comprehensive data quality report.\"\"\"\n",
        "    report = {\n",
        "        'final_row_count': len(df),\n",
        "        'final_column_count': len(df.columns),\n",
        "        'date_range': f\"{df.index.min()} to {df.index.max()}\" if hasattr(df.index, 'min') else 'N/A',\n",
        "        'scope_breakdown': df['geographic_scope'].value_counts().to_dict() if 'geographic_scope' in df.columns else 'N/A'\n",
        "    }\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        report[f'{col}_stats'] = {\n",
        "            'min': df[col].min(),\n",
        "            'max': df[col].max(), \n",
        "            'mean': df[col].mean(),\n",
        "            'null_count': df[col].isna().sum()\n",
        "        }\n",
        "    \n",
        "    logger.info(\"Final data quality report:\")\n",
        "    for key, value in report.items():\n",
        "        logger.info(f\"  {key}: {value}\")\n",
        "    \n",
        "    return report\n",
        "\n",
        "quality_report = generate_quality_report(df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Export to multiple formats for different use cases\n",
        "output_files = {}\n",
        "\n",
        "# Parquet for analysis\n",
        "parquet_path = DATA_PROCESSED / \"uk_tourism_clean.parquet\"\n",
        "df_final.to_parquet(parquet_path, index=True)\n",
        "output_files['parquet'] = parquet_path\n",
        "\n",
        "# CSV for compatibility\n",
        "csv_path = DATA_PROCESSED / \"uk_tourism_clean.csv\"\n",
        "df_final.to_csv(csv_path, index=True)\n",
        "output_files['csv'] = csv_path\n",
        "\n",
        "logger.info(\"Clean data exported to:\")\n",
        "for format_type, file_path in output_files.items():\n",
        "    logger.info(f\"  {format_type.upper()}: {file_path}\")\n",
        "\n",
        "# Create data dictionary\n",
        "data_dictionary = {\n",
        "    'quarter': 'Quarter period (Pandas Period object)',\n",
        "    'num_visits': 'Number of overseas visits (thousands)',\n",
        "    'spend_gbp_millions': 'Spending in GBP millions',\n",
        "    'num_nights': 'Number of nights stayed (millions)',\n",
        "    'geographic_scope': 'Geographical coverage: UK (2019â€“2023) or GB (2024+)' \n",
        "}\n",
        "\n",
        "dict_path = DATA_PROCESSED / \"data_dictionary.json\"\n",
        "import json\n",
        "with open(dict_path, 'w') as f:\n",
        "    json.dump(data_dictionary, f, indent=2)\n",
        "\n",
        "logger.info(f\"Data dictionary saved to: {dict_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "logger.info(\"ðŸŽ‰ Data ingestion and cleaning completed successfully!\")\n",
        "logger.info(f\"ðŸ“Š Final dataset: {len(df_final)} rows, {len(df_final.columns)} columns\")\n",
        "logger.info(f\"ðŸ“… Date range: {df_final.index.min()} to {df_final.index.max()}\")\n",
        "logger.info(\"âœ… Ready for exploratory analysis!\")"
      ]
    }
  ]
}
